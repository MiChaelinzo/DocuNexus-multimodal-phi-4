---

# DocuNexus AGI-Agent - Model Exploration

This notebook is used for experimenting with different Azure OpenAI models and configurations, including:

* **Azure OpenAI Models (e.g., GPT-4, Phi-4 Multimodal)**: Testing and comparing performance for text-based and multimodal tasks.
* **Phi-4-multimodal-instruct (Azure AI Inference)**: Comprehensive evaluation of Phi-4's multimodal capabilities within DocuNexus workflows.
* **Parameter Tuning**: Experimenting with generation parameters (e.g., temperature, top_p, etc.) to refine response quality and style for DocuNexus use cases.
* **Performance Benchmarking**: Measuring response times and resource utilization to inform model selection and optimize workflows in production.

---

## Example Explorations (Code Cells)

### **1. Azure OpenAI Text Summarization Tests**
```python
from azure.ai.openai import OpenAIClient
from azure.core.credentials import AzureKeyCredential

# Initialize Azure OpenAI Client
azure_api_key = "YOUR_AZURE_API_KEY"
azure_endpoint = "YOUR_AZURE_ENDPOINT"
client = OpenAIClient(endpoint=azure_endpoint, credential=AzureKeyCredential(azure_api_key))

# Define parameters
model_name = "gpt-4"
prompt = "Summarize the following document: [Insert document text here]"
temperature = 0.7

# Generate summary
response = client.get_chat_completions(
    model=model_name,
    messages=[{"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": prompt}],
    temperature=temperature,
    max_tokens=2000
)
print("Summary:", response.choices[0].message["content"])
```

---

### **2. Phi-4 Multimodal Image Captioning Tests**
```python
from azure.ai.openai import OpenAIClient
from azure.core.credentials import AzureKeyCredential

# Initialize Azure OpenAI Client for Phi-4
phi_model_name = "Phi-4-multimodal-instruct"
phi_client = OpenAIClient(endpoint=azure_endpoint, credential=AzureKeyCredential(azure_api_key))

# Example prompt and image encoding
image_data = open("sample_image.jpg", "rb").read()
prompt = "Provide a caption for this image."

# Send to Phi-4 model
response = phi_client.get_chat_completions(
    model=phi_model_name,
    messages=[
        {"role": "system", "content": "You are a vision expert."},
        {"role": "user", "content": prompt}
    ],
    inline_data={"image": {"data": image_data}},
    max_tokens=500
)
print("Image Caption:", response.choices[0].message["content"])
```

---

### **3. Comparison of Response Quality (GPT-4 vs. Phi-4)**
```python
# Example function to test the same prompt on different models
def compare_models(prompt, models):
    results = {}
    for model_name in models:
        response = client.get_chat_completions(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2000
        )
        results[model_name] = response.choices[0].message["content"]
    return results

# Define models and prompt
models_to_test = ["gpt-4", "Phi-4-multimodal-instruct"]
prompt = "Analyze this document: [Insert document text here]"

# Compare results
results = compare_models(prompt, models_to_test)
for model, output in results.items():
    print(f"Results from {model}:\n{output}\n")
```

---

## Parameter Tuning (Markdown Section)
Experiment with different parameters like `temperature`, `top_p`, `max_tokens`, etc., to optimize response quality for various tasks:

* **Text Generation**:
  - Lower temperature for deterministic outputs (e.g., summarization): `temperature=0.3`.
  - Higher temperature for creative outputs (e.g., story generation): `temperature=0.9`.

* **Multimodal Tasks**:
  - Increase `max_tokens` for detailed responses.
  - Adjust `top_p` for more focused generations.

---

## Performance Benchmarking (Markdown Section)
Use the following metrics to evaluate model performance:
1. **Latency**: Measure response time for text and multimodal tasks.
2. **Throughput**: Evaluate the number of requests processed per unit time.
3. **Resource Utilization**: Optimize usage of Azure resources to balance cost and performance.

---

**Conclusion & Model Selection Decisions:**
Based on the findings:
- **GPT-4**: Excels at text-based tasks such as summarization, reasoning, and Q&A.
- **Phi-4 Multimodal**: Ideal for multimodal tasks like image captioning and vision analysis.
- **Cost & Latency**: Choose models based on workload requirements and budget constraints.

